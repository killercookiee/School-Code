{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb107f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to create several functions in order to classificate our data \n",
    "\n",
    "#first we import the libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#this function help us to load the data with the .data format\n",
    "def load_data(filename):\n",
    "    columns = ['gender', 'speaker', 'phoneme', 'phoneme_ascii', 'F0', 'F1', 'F2', 'F3']\n",
    "    data = pd.read_csv(filename, delim_whitespace=True, names=columns)\n",
    "    return data\n",
    "\n",
    "#function to divide data in training, validation and test (80/10/10)\n",
    "def split_data(data):\n",
    "    train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# function to calculate the centroid of each phoneme\n",
    "def calculate_centroid(train_data):\n",
    "    centroids = train_data.groupby('phoneme')[['F0', 'F1', 'F2', 'F3']].mean()\n",
    "    return centroids\n",
    "\n",
    "# base classifier using distance to the centroids calculated\n",
    "def baseline_classifier(test_data, centroids):\n",
    "    predictions = []\n",
    "    for _, row in test_data.iterrows():\n",
    "        distances = centroids.apply(lambda x: distance.euclidean([row['F0'], row['F1'], row['F2'], row['F3']], x), axis=1)\n",
    "        predicted_phoneme = distances.idxmin()  # Fonema más cercano\n",
    "        predictions.append(predicted_phoneme)\n",
    "    return predictions\n",
    "\n",
    "#function to evaluate the model we are using\n",
    "def evaluate_model(test_data, predictions):\n",
    "    accuracy = accuracy_score(test_data['phoneme'], predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(classification_report(test_data['phoneme'], predictions))\n",
    "    \n",
    "#same functions with no F0 in case we need to take it out\n",
    "\n",
    "def calculate_centroid_noF0(train_data):\n",
    "    centroids = train_data.groupby('phoneme')[['F1', 'F2', 'F3']].mean()\n",
    "    return centroids\n",
    "\n",
    "# Clasificador base usando la distancia al centroide\n",
    "def baseline_classifier_noF0(test_data, centroids):\n",
    "    predictions = []\n",
    "    for _, row in test_data.iterrows():\n",
    "        distances = centroids.apply(lambda x: distance.euclidean([row['F1'], row['F2'], row['F3']], x), axis=1)\n",
    "        predicted_phoneme = distances.idxmin()\n",
    "        predictions.append(predicted_phoneme)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91d278fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data copying the directory of the file and ending with //verified_pb.data\n",
    "data = load_data('C:/Users/otelo/OneDrive - Universidad Politécnica de Madrid/Documentos/MATES UPM/4º/AML/PetersonBarney//verified_pb.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c4bcef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>speaker</th>\n",
       "      <th>phoneme</th>\n",
       "      <th>phoneme_ascii</th>\n",
       "      <th>F0</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>IY</td>\n",
       "      <td>160.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>2850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>IY</td>\n",
       "      <td>186.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>IH</td>\n",
       "      <td>203.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>2030.0</td>\n",
       "      <td>2640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>IH</td>\n",
       "      <td>192.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>2550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>EH</td>\n",
       "      <td>161.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>1870.0</td>\n",
       "      <td>2420.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>8</td>\n",
       "      <td>UH</td>\n",
       "      <td>322.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>3400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>UW</td>\n",
       "      <td>345.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>3460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>UW</td>\n",
       "      <td>334.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>3380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>10</td>\n",
       "      <td>ER</td>\n",
       "      <td>308.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>2160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>10</td>\n",
       "      <td>ER</td>\n",
       "      <td>328.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>1830.0</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1520 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  speaker  phoneme phoneme_ascii     F0     F1      F2      F3\n",
       "0          1        1        1            IY  160.0  240.0  2280.0  2850.0\n",
       "1          1        1        1            IY  186.0  280.0  2400.0  2790.0\n",
       "2          1        1        2            IH  203.0  390.0  2030.0  2640.0\n",
       "3          1        1        2            IH  192.0  310.0  1980.0  2550.0\n",
       "4          1        1        3            EH  161.0  490.0  1870.0  2420.0\n",
       "...      ...      ...      ...           ...    ...    ...     ...     ...\n",
       "1515       3       76        8            UH  322.0  610.0  1550.0  3400.0\n",
       "1516       3       76        9            UW  345.0  520.0  1250.0  3460.0\n",
       "1517       3       76        9            UW  334.0  500.0  1140.0  3380.0\n",
       "1518       3       76       10            ER  308.0  740.0  1850.0  2160.0\n",
       "1519       3       76       10            ER  328.0  660.0  1830.0  2200.0\n",
       "\n",
       "[1520 rows x 8 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "264f6334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data 80/10/10\n",
    "train_data, val_data, test_data = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9f81f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the centroids\n",
    "centroids = calculate_centroid(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b9f8d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F0</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phoneme</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>197.330508</td>\n",
       "      <td>299.974576</td>\n",
       "      <td>2636.610169</td>\n",
       "      <td>3238.983051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>199.424000</td>\n",
       "      <td>438.064000</td>\n",
       "      <td>2336.936000</td>\n",
       "      <td>2972.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187.772358</td>\n",
       "      <td>594.707317</td>\n",
       "      <td>2176.186992</td>\n",
       "      <td>2876.154472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>177.122951</td>\n",
       "      <td>795.901639</td>\n",
       "      <td>1959.229508</td>\n",
       "      <td>2723.713115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>184.081967</td>\n",
       "      <td>717.016393</td>\n",
       "      <td>1339.549180</td>\n",
       "      <td>2690.106557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>187.201681</td>\n",
       "      <td>844.218487</td>\n",
       "      <td>1207.865546</td>\n",
       "      <td>2728.378151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>185.177419</td>\n",
       "      <td>601.564516</td>\n",
       "      <td>916.185484</td>\n",
       "      <td>2699.338710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>199.778689</td>\n",
       "      <td>474.155738</td>\n",
       "      <td>1151.196721</td>\n",
       "      <td>2620.270492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200.292683</td>\n",
       "      <td>355.991870</td>\n",
       "      <td>961.918699</td>\n",
       "      <td>2597.365854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>192.059322</td>\n",
       "      <td>513.898305</td>\n",
       "      <td>1558.915254</td>\n",
       "      <td>1915.305085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 F0          F1           F2           F3\n",
       "phoneme                                                  \n",
       "1        197.330508  299.974576  2636.610169  3238.983051\n",
       "2        199.424000  438.064000  2336.936000  2972.760000\n",
       "3        187.772358  594.707317  2176.186992  2876.154472\n",
       "4        177.122951  795.901639  1959.229508  2723.713115\n",
       "5        184.081967  717.016393  1339.549180  2690.106557\n",
       "6        187.201681  844.218487  1207.865546  2728.378151\n",
       "7        185.177419  601.564516   916.185484  2699.338710\n",
       "8        199.778689  474.155738  1151.196721  2620.270492\n",
       "9        200.292683  355.991870   961.918699  2597.365854\n",
       "10       192.059322  513.898305  1558.915254  1915.305085"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02a16cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = baseline_classifier(test_data, centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62d8d51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 10,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 10,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 10,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 10,\n",
       " 5,\n",
       " 10,\n",
       " 9,\n",
       " 2,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 10,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions\n",
    "#we consider {1: 'a', 2: 'e', 3: 'i', 4: 'o', 5: 'u', 6: 'ae', 7: 'oe', 8: 'ii', 9: 'uu', 10: 'ou'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddede1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5263157894736842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.82      0.72        17\n",
      "           2       0.30      0.27      0.29        11\n",
      "           3       0.20      0.10      0.13        10\n",
      "           4       0.53      0.71      0.61        14\n",
      "           5       0.50      0.46      0.48        13\n",
      "           6       0.91      0.45      0.61        22\n",
      "           7       0.61      0.61      0.61        18\n",
      "           8       0.19      0.23      0.21        13\n",
      "           9       0.33      0.44      0.38        16\n",
      "          10       0.83      0.83      0.83        18\n",
      "\n",
      "    accuracy                           0.53       152\n",
      "   macro avg       0.50      0.49      0.49       152\n",
      "weighted avg       0.55      0.53      0.52       152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now we are able to see how correctly the model predicts the results\n",
    "evaluate_model(test_data, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb6f33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we get 0.526 (52.6%), which means that, on average, the model correctly predicts about half of the time. Since we have 10 classes, this value is above a random classifier (which would have an expected accuracy of 10%),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f963c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For CLASSES\n",
    "#Classes 1, 4, 6, 7, 10 have a good performance with F1-scores around 0.61-0.83, indicating that the model works reasonably well for these phonemes.\n",
    "#classes 2, 3, 8, 9 have problems. Especially classes 2, 3 and 8 show rather low F1-scores (0.13-0.38), which means that the model has difficulty correctly classifying these classes.\n",
    "#Clase 6: It has a high accuracy (0.91), but a low recall (0.45), suggesting that when it predicts class 6, it is usually correct, but often fails to recognize examples of that class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52acfef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#macro avg indicates model performance in all classes equally\n",
    "#weighted avg indicates the performance of the model weighted by the number of samples of each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90d90258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problemas identified:\n",
    "#1) Uneven performance between classes\n",
    "#2) Class 6: Difference between accuracy and recall, as we said before, indicates that the model correctly predicts when it tags examples such as Class 6 but does not correctly detect examples of this class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e919025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#soluciones:\n",
    "#1) Normalization\n",
    "#2) exclude irrelevant features like F0\n",
    "#3)#exclude unreliable data like the ones with asterisks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d625329e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud con datos normalizados: 0.675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.60      0.67        20\n",
      "          10       0.82      0.93      0.87        15\n",
      "           2       0.28      0.56      0.37         9\n",
      "           3       0.78      0.54      0.64        13\n",
      "           4       0.94      0.88      0.91        17\n",
      "           5       0.50      0.57      0.53         7\n",
      "           6       0.83      0.71      0.77         7\n",
      "           7       0.62      0.73      0.67        11\n",
      "           8       0.33      0.25      0.29         8\n",
      "           9       0.82      0.69      0.75        13\n",
      "\n",
      "    accuracy                           0.68       120\n",
      "   macro avg       0.67      0.65      0.65       120\n",
      "weighted avg       0.71      0.68      0.68       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1) Normalization\n",
    "\n",
    "#we need to define the baseline classifier in another way\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def baseline_classifier_4(X_train, y_train, X_test, y_test):\n",
    "    # Calculamos los centroides de cada fonema en el conjunto de entrenamiento\n",
    "    unique_phonemes = np.unique(y_train)\n",
    "    centroids = {}\n",
    "\n",
    "    for phoneme in unique_phonemes:\n",
    "        # Seleccionamos las filas de X_train que corresponden a cada fonema\n",
    "        centroids[phoneme] = X_train[y_train == phoneme].mean(axis=0)\n",
    "\n",
    "    # Convertimos el diccionario a una matriz de centroides para más eficiencia\n",
    "    centroid_matrix = np.array([centroids[phoneme] for phoneme in unique_phonemes])\n",
    "\n",
    "    # Predecimos el fonema más cercano a cada muestra en el conjunto de prueba\n",
    "    predictions = []\n",
    "    for sample in X_test:\n",
    "        # Calculamos la distancia de la muestra a cada centroide\n",
    "        distances = cdist([sample], centroid_matrix, metric='euclidean')\n",
    "        \n",
    "        # Seleccionamos el fonema con la distancia más corta (más cercano)\n",
    "        predicted_phoneme = unique_phonemes[np.argmin(distances)]\n",
    "        predictions.append(predicted_phoneme)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# separate the characteristics (F0, F1, F2, F3) and tags (phonemes) in training and test\n",
    "X_train = train_data[['F0', 'F1', 'F2', 'F3']]  # characteristics\n",
    "y_train = train_data['phoneme']  # phonemes\n",
    "X_test = test_data[['F0', 'F1', 'F2', 'F3']]  #characteristics\n",
    "y_test = test_data['phoneme']  #phonemes\n",
    "\n",
    "#create the scaler, which normalizes\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# normalize the characteristics that belong to the training section\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# normalize the characteristics that belong to the test section\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Paso 1: Obtener predicciones con el baseline classifier normalizado\n",
    "predictions_scaled = baseline_classifier_4(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "#we again evaluate the model\n",
    "accuracy_scaled = accuracy_score(y_test, predictions_scaled)\n",
    "print(f\"Exactitud con datos normalizados: {accuracy_scaled}\")\n",
    "\n",
    "#and print the report\n",
    "print(classification_report(y_test, predictions_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "242961ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESULTS: we see that normlizing the data has improved the results considerably in all aspects so we save it as the best solution until we find a better one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8311fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) exclude F0: we will use the functions we define at the beginning for data with no F0 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9dd17d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_noF0 = calculate_centroid_noF0(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ebdbb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_noF0 = baseline_classifier_noF0(test_data, centroids_noF0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1408fc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5328947368421053\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.82      0.72        17\n",
      "           2       0.30      0.27      0.29        11\n",
      "           3       0.20      0.10      0.13        10\n",
      "           4       0.53      0.71      0.61        14\n",
      "           5       0.58      0.54      0.56        13\n",
      "           6       1.00      0.50      0.67        22\n",
      "           7       0.58      0.61      0.59        18\n",
      "           8       0.13      0.15      0.14        13\n",
      "           9       0.33      0.44      0.38        16\n",
      "          10       0.83      0.83      0.83        18\n",
      "\n",
      "    accuracy                           0.53       152\n",
      "   macro avg       0.51      0.50      0.49       152\n",
      "weighted avg       0.56      0.53      0.53       152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(test_data, predictions_noF0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "27c1849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESULTS: we observe that taking out FO does not really improve much the results so next we will try our last solution, excluding unreliable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "370953b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3)EXCLUDE DATA WITH ASTERISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a9e7ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5083333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.55      0.56        20\n",
      "          10       0.74      0.93      0.82        15\n",
      "           2       0.08      0.11      0.09         9\n",
      "           3       0.40      0.31      0.35        13\n",
      "           4       0.86      0.71      0.77        17\n",
      "           5       0.30      0.43      0.35         7\n",
      "           6       0.56      0.71      0.63         7\n",
      "           7       0.75      0.55      0.63        11\n",
      "           8       0.12      0.12      0.12         8\n",
      "           9       0.40      0.31      0.35        13\n",
      "\n",
      "    accuracy                           0.51       120\n",
      "   macro avg       0.48      0.47      0.47       120\n",
      "weighted avg       0.53      0.51      0.51       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'ascii_phoneme' a string por si tiene algún valor no string\n",
    "data['phoneme_ascii'] = data['phoneme_ascii'].astype(str)\n",
    "\n",
    "# Eliminar filas donde la columna 'ascii_phoneme' contiene un asterisco al inicio\n",
    "data_cleaned = data[~data['phoneme_ascii'].str.startswith('*', na=False)]\n",
    "\n",
    "#again, we divide the data and repeat the process\n",
    "train_data, val_data, test_data = split_data(data_cleaned)\n",
    "\n",
    "centroids = calculate_centroid(train_data)\n",
    "\n",
    "predictions = baseline_classifier(test_data, centroids)\n",
    "\n",
    "evaluate_model(test_data, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77060b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>speaker</th>\n",
       "      <th>phoneme</th>\n",
       "      <th>phoneme_ascii</th>\n",
       "      <th>F0</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>IY</td>\n",
       "      <td>160.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>2850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>IY</td>\n",
       "      <td>186.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>IH</td>\n",
       "      <td>203.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>2030.0</td>\n",
       "      <td>2640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>IH</td>\n",
       "      <td>192.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>2550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>EH</td>\n",
       "      <td>161.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>1870.0</td>\n",
       "      <td>2420.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>8</td>\n",
       "      <td>UH</td>\n",
       "      <td>322.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>3400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>UW</td>\n",
       "      <td>345.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>3460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>UW</td>\n",
       "      <td>334.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>3380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>10</td>\n",
       "      <td>ER</td>\n",
       "      <td>308.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>2160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>10</td>\n",
       "      <td>ER</td>\n",
       "      <td>328.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>1830.0</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1199 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  speaker phoneme phoneme_ascii     F0     F1      F2      F3\n",
       "0          1        1       1            IY  160.0  240.0  2280.0  2850.0\n",
       "1          1        1       1            IY  186.0  280.0  2400.0  2790.0\n",
       "2          1        1       2            IH  203.0  390.0  2030.0  2640.0\n",
       "3          1        1       2            IH  192.0  310.0  1980.0  2550.0\n",
       "4          1        1       3            EH  161.0  490.0  1870.0  2420.0\n",
       "...      ...      ...     ...           ...    ...    ...     ...     ...\n",
       "1515       3       76       8            UH  322.0  610.0  1550.0  3400.0\n",
       "1516       3       76       9            UW  345.0  520.0  1250.0  3460.0\n",
       "1517       3       76       9            UW  334.0  500.0  1140.0  3380.0\n",
       "1518       3       76      10            ER  308.0  740.0  1850.0  2160.0\n",
       "1519       3       76      10            ER  328.0  660.0  1830.0  2200.0\n",
       "\n",
       "[1199 rows x 8 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "402aa87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESULT: we observe that cleaned data has 321 columns less and that the results for these data are a bit worse \n",
    "#therefore, we do not consider this a good solution to our problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "67fc27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final conclussion\n",
    "#the best solution to obtain better results is normalizing the data and excluding F0 or unreliable data is nor effective "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
