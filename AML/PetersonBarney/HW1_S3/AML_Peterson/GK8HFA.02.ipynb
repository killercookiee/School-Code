{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mate\\AppData\\Local\\Temp\\ipykernel_32436\\411425910.py:2: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv('PetersonBarney/verified_pb.data', delim_whitespace=True, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy without Normalization: 50.00%\n",
      "Validation Accuracy with Normalization: 11.67%\n",
      "Non-normalized is better, so we are using that\n",
      "Test Accuracy: 53.33%\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset from the given file\n",
    "df = pd.read_csv('PetersonBarney/verified_pb.data', delim_whitespace=True, header=None)\n",
    "\n",
    "#From the HEADER file, give the column names\n",
    "df.columns = ['Gender', 'Speaker', 'PhonemeNumber', 'PhonemeAscii', 'F0', 'F1', 'F2', 'F3']\n",
    "\n",
    "#We have to remove those ones, which contain * in them\n",
    "df = df[~df['PhonemeAscii'].str.contains('\\*')]\n",
    "\n",
    "#Features and labels, I will leave F0 in it\n",
    "X = df[['F0', 'F1', 'F2', 'F3']]\n",
    "y = df['PhonemeAscii']\n",
    "\n",
    "#Split the data into first 80% train, 20%test, then split 20% test into 10% validation and 10% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=23)\n",
    "\n",
    "#Calculate centroids without normalization\n",
    "centroids_no_norm = X_train.groupby(y_train).mean()\n",
    "\n",
    "#Classify based on the nearest centroid\n",
    "def classify(instance, centroids):\n",
    "    distances = centroids.apply(lambda x: euclidean(x, instance), axis=1)\n",
    "    return distances.idxmin()\n",
    "\n",
    "#Classify the validation set without normalization\n",
    "y_val_pred_no_norm = X_val.apply(lambda x: classify(x, centroids_no_norm), axis=1)\n",
    "val_accuracy_no_norm = accuracy_score(y_val, y_val_pred_no_norm)\n",
    "print(f'Validation Accuracy without Normalization: {val_accuracy_no_norm * 100:.2f}%')\n",
    "\n",
    "#Now scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Choose only the columns which we need in X_train_scaled\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=['F0', 'F1', 'F2', 'F3'])\n",
    "\n",
    "#Calculate the scaled centroids\n",
    "centroids_scaled = X_train_scaled_df.groupby(y_train).mean()\n",
    "\n",
    "#Now classify the validation set with normalization\n",
    "y_val_pred_scaled = pd.DataFrame(X_val_scaled, columns=['F0', 'F1', 'F2', 'F3']).apply(lambda x: classify(x, centroids_scaled), axis=1)\n",
    "val_accuracy_scaled = accuracy_score(y_val, y_val_pred_scaled)\n",
    "print(f'Validation Accuracy with Normalization: {val_accuracy_scaled * 100:.2f}%')\n",
    "\n",
    "#Let's choose which did better based on the validation test results\n",
    "if val_accuracy_scaled > val_accuracy_no_norm:\n",
    "    print(\"Normalized model did better, hence using that\")\n",
    "    centroids_final = centroids_scaled\n",
    "    X_test_final = X_test_scaled\n",
    "else:\n",
    "    print(\"Non-normalized is better, so we are using that\")\n",
    "    centroids_final = centroids_no_norm\n",
    "    X_test_final = X_test\n",
    "\n",
    "#Classify the test set with the better model\n",
    "y_test_pred = pd.DataFrame(X_test_final, columns=['F0', 'F1', 'F2', 'F3']).apply(lambda x: classify(x, centroids_final), axis=1)\n",
    "\n",
    "#Use the Accuracy as evaluation\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced model - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 3 with Validation Accuracy: 90.00%\n",
      "Test Accuracy with KNN (k=3): 87.50%\n"
     ]
    }
   ],
   "source": [
    "#Let's try KNN as our advanced model, also use the scaled data\n",
    "\n",
    "#Do the model for different k values, evaluate the validation set on different k values and choose the best one\n",
    "best_k = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "#Don't let k = 1, to prevent overfitting\n",
    "for k in range(2, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    #Predict the validation set with the kiven k value\n",
    "    y_val_pred = knn.predict(X_val_scaled)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    #If the current k is better, update our best_k value\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_k = k\n",
    "\n",
    "#Print out the best k value, which we will use in the model\n",
    "print(f'Best k: {best_k} with Validation Accuracy: {best_val_accuracy * 100:.2f}%')\n",
    "\n",
    "#Train the model on the training set with the best k\n",
    "knn_final = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_final.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Evaluate the model on the test set\n",
    "y_test_pred = knn_final.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#Print out the result\n",
    "print(f'Test Accuracy with KNN (k={best_k}): {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
