{"cells":[{"cell_type":"markdown","metadata":{"id":"PR75Kw1kMwea"},"source":["# Import"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"2MaDWIjDKsvq"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n"]},{"cell_type":"markdown","metadata":{"id":"9IcQGTfIMyjB"},"source":["# Data understanding"]},{"cell_type":"markdown","metadata":{"id":"lPCgY7LD8FnI"},"source":["## Reading the dataset (csv file) into Pandas dataframe"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"L1AmcerZLGKJ"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/Users/killercookie/Documents/GitHub/School code/School-Code/Machine lab/DataSet_LakasArak_labeled.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m housing_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/killercookie/Documents/GitHub/School code/School-Code/Machine lab/DataSet_LakasArak_labeled.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m housing \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhousing_file_path\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/killercookie/Documents/GitHub/School code/School-Code/Machine lab/DataSet_LakasArak_labeled.csv'"]}],"source":["housing_file_path = \"/Users/killercookie/Documents/GitHub/School code/School-Code/Machine lab/DataSet_LakasArak_labeled.csv\"\n","housing = pd.DataFrame(pd.read_csv(housing_file_path))"]},{"cell_type":"markdown","metadata":{"id":"x7T1kid18dDQ"},"source":["## Exploring the dataframe"]},{"cell_type":"code","execution_count":186,"metadata":{"id":"LvhaRTjuL23B"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>county</th>\n","      <th>city</th>\n","      <th>postcode</th>\n","      <th>property_type</th>\n","      <th>property_subtype</th>\n","      <th>property_condition_type</th>\n","      <th>property_floor</th>\n","      <th>building_floor_count</th>\n","      <th>view_type</th>\n","      <th>orientation</th>\n","      <th>...</th>\n","      <th>room_cnt</th>\n","      <th>small_room_cnt</th>\n","      <th>created_at</th>\n","      <th>property_area</th>\n","      <th>balcony_area</th>\n","      <th>price_created_at</th>\n","      <th>ad_view_cnt</th>\n","      <th>active_days</th>\n","      <th>nr</th>\n","      <th>split</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Budapest</td>\n","      <td>Budapest XII.</td>\n","      <td>NaN</td>\n","      <td>flat</td>\n","      <td>brick flat (for sale)</td>\n","      <td>good</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>street view</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2015-02-09</td>\n","      <td>65.0</td>\n","      <td>0.0</td>\n","      <td>23.5</td>\n","      <td>605.0</td>\n","      <td>119.0</td>\n","      <td>4</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Budapest</td>\n","      <td>Budapest I.</td>\n","      <td>1016.0</td>\n","      <td>flat</td>\n","      <td>brick flat (for sale)</td>\n","      <td>novel</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>street view</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2015-02-09</td>\n","      <td>45.0</td>\n","      <td>0.0</td>\n","      <td>20.0</td>\n","      <td>49.0</td>\n","      <td>25.0</td>\n","      <td>12</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Budapest</td>\n","      <td>Budapest XVI.</td>\n","      <td>1164.0</td>\n","      <td>flat</td>\n","      <td>brick flat (for sale)</td>\n","      <td>novel</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>garden view</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2015-02-09</td>\n","      <td>60.0</td>\n","      <td>0.0</td>\n","      <td>22.0</td>\n","      <td>77.0</td>\n","      <td>77.0</td>\n","      <td>14</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Budapest</td>\n","      <td>Budapest X.</td>\n","      <td>NaN</td>\n","      <td>flat</td>\n","      <td>brick flat (for sale)</td>\n","      <td>good</td>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>garden view</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2015-02-09</td>\n","      <td>55.0</td>\n","      <td>4.0</td>\n","      <td>11.0</td>\n","      <td>139.0</td>\n","      <td>18.0</td>\n","      <td>21</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Budapest</td>\n","      <td>Budapest XVIII.</td>\n","      <td>1181.0</td>\n","      <td>flat</td>\n","      <td>prefabricated panel flat (for sale)</td>\n","      <td>renewed</td>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2015-02-09</td>\n","      <td>60.0</td>\n","      <td>3.0</td>\n","      <td>10.2</td>\n","      <td>176.0</td>\n","      <td>69.0</td>\n","      <td>31</td>\n","      <td>test</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 23 columns</p>\n","</div>"],"text/plain":["     county             city  postcode property_type  \\\n","0  Budapest    Budapest XII.       NaN          flat   \n","1  Budapest      Budapest I.    1016.0          flat   \n","2  Budapest    Budapest XVI.    1164.0          flat   \n","3  Budapest      Budapest X.       NaN          flat   \n","4  Budapest  Budapest XVIII.    1181.0          flat   \n","\n","                      property_subtype property_condition_type property_floor  \\\n","0                brick flat (for sale)                    good              1   \n","1                brick flat (for sale)                   novel              2   \n","2                brick flat (for sale)                   novel              1   \n","3                brick flat (for sale)                    good              4   \n","4  prefabricated panel flat (for sale)                 renewed              6   \n","\n","  building_floor_count    view_type orientation  ... room_cnt small_room_cnt  \\\n","0                  NaN  street view         NaN  ...      2.0            1.0   \n","1                  NaN  street view         NaN  ...      1.0            1.0   \n","2                  NaN  garden view         NaN  ...      2.0            1.0   \n","3                  NaN  garden view         NaN  ...      2.0            0.0   \n","4                  NaN          NaN         NaN  ...      2.0            1.0   \n","\n","   created_at  property_area  balcony_area price_created_at  ad_view_cnt  \\\n","0  2015-02-09           65.0           0.0             23.5        605.0   \n","1  2015-02-09           45.0           0.0             20.0         49.0   \n","2  2015-02-09           60.0           0.0             22.0         77.0   \n","3  2015-02-09           55.0           4.0             11.0        139.0   \n","4  2015-02-09           60.0           3.0             10.2        176.0   \n","\n","   active_days  nr  split  \n","0        119.0   4   test  \n","1         25.0  12  train  \n","2         77.0  14  train  \n","3         18.0  21  train  \n","4         69.0  31   test  \n","\n","[5 rows x 23 columns]"]},"execution_count":186,"metadata":{},"output_type":"execute_result"}],"source":["# Check the head of the dataset\n","housing.head()"]},{"cell_type":"code","execution_count":187,"metadata":{"id":"Ge4N2njkL5oY"},"outputs":[{"data":{"text/plain":["(78539, 23)"]},"execution_count":187,"metadata":{},"output_type":"execute_result"}],"source":["housing.shape"]},{"cell_type":"code","execution_count":188,"metadata":{"id":"i6y5AhBE98v-"},"outputs":[],"source":["# The dataset shape shows the number of instances and features in the dataset\n","num_instances = housing.shape[0]  # Number of rows\n","num_features = housing.shape[1]   # Number of columns\n"]},{"cell_type":"code","execution_count":189,"metadata":{"id":"GHB3VZtzL-tQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 78539 entries, 0 to 78538\n","Data columns (total 23 columns):\n"," #   Column                   Non-Null Count  Dtype  \n","---  ------                   --------------  -----  \n"," 0   county                   78539 non-null  object \n"," 1   city                     77980 non-null  object \n"," 2   postcode                 49585 non-null  float64\n"," 3   property_type            78539 non-null  object \n"," 4   property_subtype         76880 non-null  object \n"," 5   property_condition_type  78539 non-null  object \n"," 6   property_floor           74746 non-null  object \n"," 7   building_floor_count     36429 non-null  object \n"," 8   view_type                42878 non-null  object \n"," 9   orientation              47647 non-null  object \n"," 10  garden_access            17200 non-null  object \n"," 11  heating_type             67233 non-null  object \n"," 12  elevator_type            64388 non-null  object \n"," 13  room_cnt                 78539 non-null  float64\n"," 14  small_room_cnt           78539 non-null  float64\n"," 15  created_at               78539 non-null  object \n"," 16  property_area            78539 non-null  float64\n"," 17  balcony_area             78539 non-null  float64\n"," 18  price_created_at         78539 non-null  float64\n"," 19  ad_view_cnt              78539 non-null  float64\n"," 20  active_days              78539 non-null  float64\n"," 21  nr                       78539 non-null  int64  \n"," 22  split                    78539 non-null  object \n","dtypes: float64(8), int64(1), object(14)\n","memory usage: 13.8+ MB\n"]}],"source":["housing.info()"]},{"cell_type":"code","execution_count":190,"metadata":{"id":"XSbEbKRSMBfx"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>postcode</th>\n","      <th>room_cnt</th>\n","      <th>small_room_cnt</th>\n","      <th>property_area</th>\n","      <th>balcony_area</th>\n","      <th>price_created_at</th>\n","      <th>ad_view_cnt</th>\n","      <th>active_days</th>\n","      <th>nr</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>49585.000000</td>\n","      <td>78539.000000</td>\n","      <td>78539.000000</td>\n","      <td>78539.000000</td>\n","      <td>78539.000000</td>\n","      <td>78539.000000</td>\n","      <td>78539.000000</td>\n","      <td>78539.000000</td>\n","      <td>78539.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>1103.358980</td>\n","      <td>1.467666</td>\n","      <td>0.559875</td>\n","      <td>48.440584</td>\n","      <td>1.953182</td>\n","      <td>19.341475</td>\n","      <td>259.599320</td>\n","      <td>44.173684</td>\n","      <td>196334.093240</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>50.769326</td>\n","      <td>0.599840</td>\n","      <td>0.737015</td>\n","      <td>12.716653</td>\n","      <td>4.677227</td>\n","      <td>8.900296</td>\n","      <td>512.351553</td>\n","      <td>47.821006</td>\n","      <td>113305.083861</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1011.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>5.000000</td>\n","      <td>0.000000</td>\n","      <td>0.200000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1064.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>40.000000</td>\n","      <td>0.000000</td>\n","      <td>13.200000</td>\n","      <td>42.000000</td>\n","      <td>11.000000</td>\n","      <td>97959.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1101.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>50.000000</td>\n","      <td>0.000000</td>\n","      <td>16.900000</td>\n","      <td>103.000000</td>\n","      <td>28.000000</td>\n","      <td>196095.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1142.000000</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>3.000000</td>\n","      <td>23.900000</td>\n","      <td>263.000000</td>\n","      <td>61.000000</td>\n","      <td>294516.500000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1239.000000</td>\n","      <td>7.000000</td>\n","      <td>4.000000</td>\n","      <td>70.000000</td>\n","      <td>97.000000</td>\n","      <td>99.600000</td>\n","      <td>28096.000000</td>\n","      <td>537.000000</td>\n","      <td>394181.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           postcode      room_cnt  small_room_cnt  property_area  \\\n","count  49585.000000  78539.000000    78539.000000   78539.000000   \n","mean    1103.358980      1.467666        0.559875      48.440584   \n","std       50.769326      0.599840        0.737015      12.716653   \n","min     1011.000000      0.000000        0.000000       5.000000   \n","25%     1064.000000      1.000000        0.000000      40.000000   \n","50%     1101.000000      1.000000        0.000000      50.000000   \n","75%     1142.000000      2.000000        1.000000      60.000000   \n","max     1239.000000      7.000000        4.000000      70.000000   \n","\n","       balcony_area  price_created_at   ad_view_cnt   active_days  \\\n","count  78539.000000      78539.000000  78539.000000  78539.000000   \n","mean       1.953182         19.341475    259.599320     44.173684   \n","std        4.677227          8.900296    512.351553     47.821006   \n","min        0.000000          0.200000      0.000000      1.000000   \n","25%        0.000000         13.200000     42.000000     11.000000   \n","50%        0.000000         16.900000    103.000000     28.000000   \n","75%        3.000000         23.900000    263.000000     61.000000   \n","max       97.000000         99.600000  28096.000000    537.000000   \n","\n","                  nr  \n","count   78539.000000  \n","mean   196334.093240  \n","std    113305.083861  \n","min         4.000000  \n","25%     97959.000000  \n","50%    196095.000000  \n","75%    294516.500000  \n","max    394181.000000  "]},"execution_count":190,"metadata":{},"output_type":"execute_result"}],"source":["housing.describe()"]},{"cell_type":"markdown","metadata":{"id":"8jHSic1P-9_1"},"source":["## Features and ground truth labels"]},{"cell_type":"code","execution_count":191,"metadata":{"id":"XXok0q5X_DeT"},"outputs":[],"source":["# One of the columns contains the prices. In this task, we need to predict the prices based on some information that we have; thus, this column is the ground truth label.\n","# 'price_created_at' column has the ground truth label that we are going to use in training and testing later.\n","gt_feature = 'price_created_at'"]},{"cell_type":"markdown","metadata":{"id":"-cP3kjc1MN5N"},"source":["# Data preperation"]},{"cell_type":"code","execution_count":192,"metadata":{"id":"LPIWK-zjMN5U"},"outputs":[],"source":["# As we learned earlier, prepare the data for the next steps (e.g. train and test).\n","# You might use all the available features or part of them. Please justify your choices.\n","# Attention!!! Do not drop any N/A value.\n","# ...\n","\n","\n","# Function to impute missing textual data while preserving distribution\n","def impute_categorical_with_distribution(df, columns):\n","    for col in columns:\n","        if df[col].isnull().sum() > 0:\n","            # Get the frequency distribution of the non-null values\n","            value_counts = df[col].value_counts(normalize=True)\n","            # Impute missing values by sampling from the observed distribution\n","            imputed_values = np.random.choice(value_counts.index, \n","                                              size=df[col].isnull().sum(), \n","                                              p=value_counts.values)\n","            df.loc[df[col].isnull(), col] = imputed_values\n","    \n","    return df\n","\n","# List of categorical/textual columns\n","categorical_columns = housing.select_dtypes(include=['object']).columns\n","housing = impute_categorical_with_distribution(housing, categorical_columns)\n","\n","# Function to impute missing values based on skewness\n","def impute_numerical_with_distribution(df, columns):\n","    for col in columns:\n","        if df[col].isnull().sum() > 0:\n","            skewness = df[col].skew()\n","            if abs(skewness) < 0.5:  # Low skewness, use mean\n","                mean_value = df[col].mean()\n","                noise = np.random.normal(loc=0, scale=df[col].std(), size=df[col].isnull().sum())\n","                df.loc[df[col].isnull(), col] = mean_value + noise\n","            else:  # High skewness, use median\n","                median_value = df[col].median()\n","                # Sample from the observed distribution and add a bit of randomness\n","                observed_values = df[col].dropna()\n","                imputed_values = np.random.choice(observed_values, size=df[col].isnull().sum())\n","                df.loc[df[col].isnull(), col] = imputed_values\n","\n","    return df\n","\n","# List of numerical columns\n","numerical_columns = housing.select_dtypes(include=['float64', 'int64']).columns\n","housing = impute_numerical_with_distribution(housing, numerical_columns)\n","\n","# File path where the new dataset will be saved\n","new_file_path = \"/Users/killercookie/Documents/GitHub/School code/School-Code/Machine lab/new_dataset.csv\"\n","\n","# Save the modified dataset to a CSV file\n","housing.to_csv(new_file_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"8cuT5ysJMN5U"},"source":["## Holding out a test set for performance evaluation"]},{"cell_type":"code","execution_count":193,"metadata":{"id":"JV8bX6LwMN5U"},"outputs":[{"data":{"text/plain":["((54977, 23), (23562, 23))"]},"execution_count":193,"metadata":{},"output_type":"execute_result"}],"source":["# 1- We need to decide how much of the data is used for testing.\n","#    In this experiment the data is labeled beforehand, we have 30% of the data for testing purposes.\n","# 2- How many instances do we have for training and testing?\n","\n","train_set = housing[housing['split']=='train']\n","test_set = housing[housing['split']=='test']\n","\n","train_set.shape, test_set.shape"]},{"cell_type":"code","execution_count":194,"metadata":{"id":"btwvyP_ILMaE"},"outputs":[],"source":["# The following is just to assert that the data is complete and none of th einstances was dropped\n","test_perc = 0.3\n","train_perc = 1 - test_perc\n","\n","assert (len(train_set) + len(test_set)) == num_instances\n","assert (len(train_set)) == int(train_perc*num_instances)\n","assert (len(test_set)) == (num_instances - len(train_set))"]},{"cell_type":"markdown","metadata":{"id":"NvQBZNegCw7Y"},"source":["# Model selection"]},{"cell_type":"code","execution_count":195,"metadata":{"id":"ZQvcLguWEQDs"},"outputs":[{"name":"stdout","output_type":"stream","text":["['postcode', 'room_cnt', 'small_room_cnt', 'property_area', 'balcony_area', 'ad_view_cnt', 'active_days', 'nr']\n"]}],"source":["# After the data preperation/preprocessing step, the list of selected features (as strings) should be saved into a list in the form:\n","# features = [feature1, feature2, ...]\n","# Select only numerical features (int64 and float64 types)\n","\n","numerical_features = housing.select_dtypes(include=['int64', 'float64']).columns\n","\n","# Exclude the target column (price) and any non-feature columns like 'split'\n","features = [col for col in numerical_features if col != 'price_created_at']\n","print(features)"]},{"cell_type":"code","execution_count":196,"metadata":{"id":"RaJZt1NOFVNM"},"outputs":[{"name":"stdout","output_type":"stream","text":["(54977, 8) (54977,) (23562, 8) (23562,)\n"]}],"source":["# We need to create features and ground truth sets for both train and test splits that we have. Use 'features' and 'gt_feature'.\n","\n","X_train = train_set[features]\n","y_train = train_set[gt_feature]\n","\n","X_test = test_set[features]\n","y_test = test_set[gt_feature]\n","\n","print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"]},{"cell_type":"code","execution_count":197,"metadata":{"id":"CO7Zi-7qC5up"},"outputs":[],"source":["# For prediction, you should use the following function. As you can see, it is incomplete, please fill the gaps.\n","# The selected model has to learn the features in the data before giving an educated prediction. Why?\n","# We first fit the model using the train data, then use it to predict labels (prices) for the test instances. Why?\n","\n","def model_predict(model, X_train, y_train, X_test):\n","  # fit the model\n","  model.fit(X_train, y_train)\n","\n","  # make predictions\n","  pred = model.predict(X_test)\n","  return model, pred"]},{"cell_type":"code","execution_count":198,"metadata":{"id":"VbGzNzUoFRSB"},"outputs":[{"name":"stdout","output_type":"stream","text":["[22.31898931 17.67666373 11.54571735 13.71106863 22.02962761] [21.299 15.663  9.53  17.127 17.881] [21.22572185 15.73415942  9.74853171 17.14417257 18.80181525]\n"]}],"source":["# Using the selected models, You can make the predictions using 'model_predict' function. Please save the returned values so we can check their performance.\n","# ...\n","\n","# Initialize models\n","model_1 = LinearRegression()\n","model_2 = RandomForestRegressor()\n","model_3 = GradientBoostingRegressor()\n","\n","# Make predictions using the defined model_predict function\n","model_1, pred_1 = model_predict(model_1, X_train, y_train, X_test)\n","model_2, pred_2 = model_predict(model_2, X_train, y_train, X_test)\n","model_3, pred_3 = model_predict(model_3, X_train, y_train, X_test)\n","\n","# Check predictions (optional)\n","print(pred_1[:5], pred_2[:5], pred_3[:5])"]},{"cell_type":"markdown","metadata":{"id":"ukbVGjUvCyoG"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":199,"metadata":{"id":"qkwNqkPoC0h6"},"outputs":[],"source":["# For evaluation we use Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and Root Mean Square Error (RMSE).\n","# Please complete the following function:\n","\n","def model_evaluate(pred, target):\n","    mae = mean_absolute_error(target, pred)\n","    mape = np.mean(np.abs((target - pred) / target)) * 100  # MAPE calculation\n","    rmse = np.sqrt(mean_squared_error(target, pred))\n","    \n","    return mae, mape, rmse"]},{"cell_type":"code","execution_count":200,"metadata":{"id":"rAJ5I2RvC0ff"},"outputs":[],"source":["# Check and compare the performance for all the models. Do you find any interesting observtions(s)? What are your conclusion(s)?"]},{"cell_type":"code","execution_count":201,"metadata":{"id":"4PA__ov2C0DU"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model 1 (Linear Regression): MAE=5.356422305362718, MAPE=29.388705395841235, RMSE=7.4067527226305065\n","Model 2 (Random Forest): MAE=3.889860241066124, MAPE=21.364579499769, RMSE=5.926298361187138\n","Model 3 (Gradient Boosting): MAE=4.391367562711816, MAPE=24.100257216727986, RMSE=6.333144479104173\n"]}],"source":["# Evaluate model 1 (Linear Regression)\n","mae_1, mape_1, rmse_1 = model_evaluate(pred_1, y_test)\n","print(f\"Model 1 (Linear Regression): MAE={mae_1}, MAPE={mape_1}, RMSE={rmse_1}\")\n","\n","# Evaluate model 2 (Random Forest)\n","mae_2, mape_2, rmse_2 = model_evaluate(pred_2, y_test)\n","print(f\"Model 2 (Random Forest): MAE={mae_2}, MAPE={mape_2}, RMSE={rmse_2}\")\n","\n","# Evaluate model 3 (Gradient Boosting)\n","mae_3, mape_3, rmse_3 = model_evaluate(pred_3, y_test)\n","print(f\"Model 3 (Gradient Boosting): MAE={mae_3}, MAPE={mape_3}, RMSE={rmse_3}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMAw6Lxm7wnVvDiYgwdVQxV","mount_file_id":"19XXXifDc4l_Sq8Hv8aIi8penr4Udhudk","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
